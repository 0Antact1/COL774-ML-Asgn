{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Competitive Part - Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=5)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=5)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # def forward(self, input, hidden, cell):\n",
    "    #     input = input.unsqueeze(0)\n",
    "    #     embedded = self.embedding(input)\n",
    "    #     output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "    #     output = self.softmax(self.out(output[0]))\n",
    "    #     return output, hidden, cell\n",
    "\n",
    "    def forward(self, context, input, hidden):\n",
    "        context = context.squeeze().unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # print(\"Context\", context.shape)\n",
    "        # print(\"Embed\", embedded.shape)\n",
    "\n",
    "        embedded = torch.cat((context, embedded), dim=2)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # print(\"Out\", output.shape)\n",
    "        # output = output.squeeze(1)\n",
    "        output = self.out(output)\n",
    "        # print(\"Outnew\", output.shape)\n",
    "        # print(hidden)\n",
    "        # output = self.softmax(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([12,1,14,213,41,121])\n",
    "torch.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 12,   1,  14, 213,  41, 121])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(nn.Module):\n",
    "    def __init__(self, vocab, word_to_index, index_to_word, hidden_size=512, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(len(vocab), hidden_size, embedding_dim)\n",
    "        self.out_size = len(vocab)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "\n",
    "    def forward(self, images, str_input, hidden):\n",
    "        context_img = self.encoder.forward(images)\n",
    "        batch_size, _, _, _ = context_img.size()\n",
    "\n",
    "        context_img = context_img.view(batch_size, -1)\n",
    "        output, _ = self.decoder.forward(context_img, str_input, hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generating vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./col_774_A4_2023/SyntheticData/train.csv')\n",
    "\n",
    "# added Start and End tokens\n",
    "vocab = set()\n",
    "for formula in df['formula']:\n",
    "    tokens = formula.split()\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {'<PAD>':0, '<SOF>':1, '<EOF>':2}\n",
    "word_to_index.update({word: i+3 for i, word in enumerate(vocab)})\n",
    "\n",
    "vocab.update({'<SOF>', '<EOF>', '<PAD>'})\n",
    "\n",
    "embed_dim = 512\n",
    "embed_matrix = nn.Embedding(len(vocab), embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix(torch.tensor(word_to_index['<SOF>'])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = [word_to_index[word] for word in df['formula'][1].split()]\n",
    "\n",
    "embed_vec = embed_matrix(torch.tensor(trial))\n",
    "embed_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<SOF>',\n",
       " 2: '<EOF>',\n",
       " 3: 'P',\n",
       " 4: '\\\\slash',\n",
       " 5: '\\\\sim',\n",
       " 6: '2',\n",
       " 7: '3.2',\n",
       " 8: '\\\\ddagger',\n",
       " 9: '\\\\bullet',\n",
       " 10: '\\\\coprod',\n",
       " 11: '\\\\vdash',\n",
       " 12: '\\\\vector',\n",
       " 13: '\\\\mathrm',\n",
       " 14: '\\\\thicklines',\n",
       " 15: ')',\n",
       " 16: '\\\\mid',\n",
       " 17: '\\\\partial',\n",
       " 18: '\\\\makebox',\n",
       " 19: '\\\\symbol',\n",
       " 20: '\\\\Biggl',\n",
       " 21: 'N',\n",
       " 22: '\\\\cup',\n",
       " 23: '\\\\S',\n",
       " 24: '\\\\tabcolsep',\n",
       " 25: '0.1',\n",
       " 26: '^',\n",
       " 27: '\\\\underline',\n",
       " 28: '?',\n",
       " 29: '\\\\overline',\n",
       " 30: '\\\\arraycolsep',\n",
       " 31: '\\\\textnormal',\n",
       " 32: '\\\\int',\n",
       " 33: '\\\\ddot',\n",
       " 34: '\\\\',\n",
       " 35: '\\\\subset',\n",
       " 36: '\\\\#',\n",
       " 37: '\\\\fill',\n",
       " 38: '\\\\Longleftrightarrow',\n",
       " 39: '\\\\Bigm',\n",
       " 40: 'R',\n",
       " 41: '\\\\otimes',\n",
       " 42: '\\\\tt',\n",
       " 43: '\\\\stackrel',\n",
       " 44: 'K',\n",
       " 45: '\\\\longrightarrow',\n",
       " 46: '\\\\fbox',\n",
       " 47: '\\\\overwithdelims',\n",
       " 48: '\\\\mp',\n",
       " 49: '\\\\hline',\n",
       " 50: 'm',\n",
       " 51: '\\\\bigtriangledown',\n",
       " 52: '\\\\ll',\n",
       " 53: '\\\\label',\n",
       " 54: '\\\\everymath',\n",
       " 55: '\\\\scriptstyle',\n",
       " 56: '\\\\prime',\n",
       " 57: '\\\\textcircled',\n",
       " 58: 'n',\n",
       " 59: '\\\\ddag',\n",
       " 60: '\\\\AA',\n",
       " 61: '\\\\phi',\n",
       " 62: '\\\\tau',\n",
       " 63: '0.9',\n",
       " 64: '\\\\sqcup',\n",
       " 65: '\\\\mathversion',\n",
       " 66: '\\\\mapsto',\n",
       " 67: '\\\\ge',\n",
       " 68: '\\\\\"',\n",
       " 69: '\\\\lower',\n",
       " 70: '\\\\flat',\n",
       " 71: '\\\\boldmath',\n",
       " 72: '\\\\downarrow',\n",
       " 73: '\\\\Downarrow',\n",
       " 74: '\\\\widetilde',\n",
       " 75: '[',\n",
       " 76: '\\\\vspace',\n",
       " 77: '~',\n",
       " 78: '\\\\subseteq',\n",
       " 79: '\\\\parallel',\n",
       " 80: '\\\\]',\n",
       " 81: '\\\\mathbin',\n",
       " 82: '\\\\protectm',\n",
       " 83: 'L',\n",
       " 84: '\\\\-',\n",
       " 85: '\\\\rbrace',\n",
       " 86: 'pt',\n",
       " 87: '\\\\smile',\n",
       " 88: '/',\n",
       " 89: '\\\\chi',\n",
       " 90: '\\\\Gamma',\n",
       " 91: '\\\\grave',\n",
       " 92: '\\\\textstyle',\n",
       " 93: '\\\\newcommand',\n",
       " 94: '\\\\_',\n",
       " 95: '\\\\rightleftharpoons',\n",
       " 96: '}',\n",
       " 97: '\\\\L',\n",
       " 98: '\\\\vdots',\n",
       " 99: '\\\\psi',\n",
       " 100: '\\\\left\\\\Vert',\n",
       " 101: 'A',\n",
       " 102: '\\\\triangle',\n",
       " 103: '\\\\left\\\\lbrace',\n",
       " 104: 'Z',\n",
       " 105: '\\\\i',\n",
       " 106: '\\\\left\\\\langle',\n",
       " 107: '\\\\leftarrow',\n",
       " 108: '\\\\left\\\\lfloor',\n",
       " 109: '\\\\biggl',\n",
       " 110: '\\\\eta',\n",
       " 111: '\\\\cong',\n",
       " 112: '\\\\bigcap',\n",
       " 113: 'b',\n",
       " 114: '\\\\mathsf',\n",
       " 115: '\\\\l',\n",
       " 116: '\\\\kern',\n",
       " 117: '\\\\end{array}',\n",
       " 118: '\\\\bigwedge',\n",
       " 119: '\\\\alpha',\n",
       " 120: '\\\\hbar',\n",
       " 121: '\\\\preceq',\n",
       " 122: '\\\\lefteqn',\n",
       " 123: 'S',\n",
       " 124: '3',\n",
       " 125: '\\\\begin{array}',\n",
       " 126: '\\\\parbox',\n",
       " 127: '\\\\\\\\',\n",
       " 128: '\\\\hrule',\n",
       " 129: '\\\\bigotimes',\n",
       " 130: '\\\\Big',\n",
       " 131: '\\\\triangleright',\n",
       " 132: 'D',\n",
       " 133: '\\\\vec',\n",
       " 134: '\\\\[',\n",
       " 135: '\\\\atopwithdelims',\n",
       " 136: '\\\\cdot',\n",
       " 137: '\\\\j',\n",
       " 138: '\\\\rightarrow',\n",
       " 139: '\\\\ell',\n",
       " 140: '\\\\{',\n",
       " 141: 's',\n",
       " 142: '\\\\footnotemark',\n",
       " 143: 'X',\n",
       " 144: '8',\n",
       " 145: '\\\\bf',\n",
       " 146: 'Q',\n",
       " 147: '9',\n",
       " 148: '\\\\Xi',\n",
       " 149: '\\\\simeq',\n",
       " 150: '\\\\uparrow',\n",
       " 151: '\\\\cline',\n",
       " 152: 'H',\n",
       " 153: 'k',\n",
       " 154: '\\\\omega',\n",
       " 155: '\\\\rangle',\n",
       " 156: '\\\\nonumber',\n",
       " 157: '\\\\smallskip',\n",
       " 158: '-',\n",
       " 159: '\\\\pounds',\n",
       " 160: '\\\\Biggr',\n",
       " 161: '\\\\small',\n",
       " 162: '\\\\backslash',\n",
       " 163: 'l',\n",
       " 164: '\\\\llap',\n",
       " 165: '\\\\Phi',\n",
       " 166: \"'\",\n",
       " 167: '\\\\bigoplus',\n",
       " 168: '\\\\P',\n",
       " 169: '\\\\infty',\n",
       " 170: '\\\\enspace',\n",
       " 171: '\\\\b',\n",
       " 172: '\\\\bigm',\n",
       " 173: '\\\\right]',\n",
       " 174: '\\\\|',\n",
       " 175: '\\\\Bigg',\n",
       " 176: '\\\\star',\n",
       " 177: 'v',\n",
       " 178: '\\\\asymp',\n",
       " 179: 'd',\n",
       " 180: '#',\n",
       " 181: '\\\\left]',\n",
       " 182: '\\\\dot',\n",
       " 183: '\\\\protectE',\n",
       " 184: '\\\\right\\\\Vert',\n",
       " 185: '\\\\bigcirc',\n",
       " 186: 'h',\n",
       " 187: '.',\n",
       " 188: 'J',\n",
       " 189: '\\\\longleftarrow',\n",
       " 190: '\\\\left[',\n",
       " 191: '\\\\bigg',\n",
       " 192: '6',\n",
       " 193: '\\\\triangleleft',\n",
       " 194: '\\\\smallint',\n",
       " 195: '\\\\scshape',\n",
       " 196: '\\\\Bigr',\n",
       " 197: '\\\\Delta',\n",
       " 198: '\\\\supset',\n",
       " 199: '\\\\special',\n",
       " 200: '\\\\raise',\n",
       " 201: '\\\\binom',\n",
       " 202: '\\\\longleftrightarrow',\n",
       " 203: 'O',\n",
       " 204: '\\\\cite',\n",
       " 205: '\\\\gamma',\n",
       " 206: 't',\n",
       " 207: '\\\\mathopen',\n",
       " 208: '@',\n",
       " 209: '<',\n",
       " 210: '\\\\raisebox',\n",
       " 211: '\\\\bot',\n",
       " 212: '\\\\left|',\n",
       " 213: '\\\\qquad',\n",
       " 214: '\\\\rbrack',\n",
       " 215: '\\\\Sigma',\n",
       " 216: '\\\\left\\\\vert',\n",
       " 217: '\\\\widehat',\n",
       " 218: '\\\\mit',\n",
       " 219: '\\\\delta',\n",
       " 220: '\\\\dagger',\n",
       " 221: '\\\\protectu',\n",
       " 222: '0.23',\n",
       " 223: '\\\\acute',\n",
       " 224: '\\\\to',\n",
       " 225: '\\\\em',\n",
       " 226: '\\\\varepsilon',\n",
       " 227: '\\\\surd',\n",
       " 228: '\\\\exists',\n",
       " 229: '\\\\varphi',\n",
       " 230: '\\\\times',\n",
       " 231: '\\\\circle',\n",
       " 232: '\\\\prod',\n",
       " 233: '\\\\right|',\n",
       " 234: '\\\\operatorname*',\n",
       " 235: '0.5',\n",
       " 236: '\\\\bigsqcup',\n",
       " 237: '\\\\pmod',\n",
       " 238: '\\\\varrho',\n",
       " 239: '$',\n",
       " 240: '\\\\big',\n",
       " 241: '\\\\overleftarrow',\n",
       " 242: '\\\\mathaccent',\n",
       " 243: '\\\\check',\n",
       " 244: '10',\n",
       " 245: '\\\\enskip',\n",
       " 246: '\"',\n",
       " 247: '\\\\mathbf',\n",
       " 248: '\\\\renewcommand',\n",
       " 249: '\\\\ast',\n",
       " 250: '\\\\Theta',\n",
       " 251: '\\\\equiv',\n",
       " 252: '\\\\hfill',\n",
       " 253: '\\\\LARGE',\n",
       " 254: '\\\\sp',\n",
       " 255: '\\\\skew',\n",
       " 256: 'x',\n",
       " 257: '\\\\^',\n",
       " 258: '\\\\ominus',\n",
       " 259: '\\\\Vert',\n",
       " 260: '\\\\vss',\n",
       " 261: '4',\n",
       " 262: '\\\\Lambda',\n",
       " 263: '&',\n",
       " 264: '\\\\dots',\n",
       " 265: 'u',\n",
       " 266: 'e',\n",
       " 267: '\\\\perp',\n",
       " 268: '\\\\m',\n",
       " 269: 'y',\n",
       " 270: '\\\\left(',\n",
       " 271: '\\\\overrightarrow',\n",
       " 272: '\\\\unboldmath',\n",
       " 273: '\\\\large',\n",
       " 274: '\\\\Im',\n",
       " 275: '7',\n",
       " 276: '\\\\emph',\n",
       " 277: '\\\\ooalign',\n",
       " 278: '\\\\pm',\n",
       " 279: '\\\\sqcap',\n",
       " 280: 'z',\n",
       " 281: 'V',\n",
       " 282: '\\\\scriptscriptstyle',\n",
       " 283: \"\\\\'\",\n",
       " 284: '\\\\texttt',\n",
       " 285: '\\\\unitlength',\n",
       " 286: '\\\\diamond',\n",
       " 287: '\\\\right\\\\rfloor',\n",
       " 288: '(',\n",
       " 289: '\\\\sc',\n",
       " 290: '\\\\nulldelimiterspace',\n",
       " 291: 'U',\n",
       " 292: '\\\\colon',\n",
       " 293: 'E',\n",
       " 294: '\\\\Huge',\n",
       " 295: 'o',\n",
       " 296: '\\\\wp',\n",
       " 297: '\\\\hss',\n",
       " 298: '\\\\not',\n",
       " 299: '\\\\;',\n",
       " 300: '\\\\propto',\n",
       " 301: '>',\n",
       " 302: '\\\\arraystretch',\n",
       " 303: '\\\\space',\n",
       " 304: '0.14',\n",
       " 305: '\\\\lfloor',\n",
       " 306: '\\\\nolinebreak',\n",
       " 307: '\\\\Longleftarrow',\n",
       " 308: '\\\\sb',\n",
       " 309: '\\\\vskip',\n",
       " 310: '\\\\normalsize',\n",
       " 311: '\\\\mu',\n",
       " 312: 'cm',\n",
       " 313: '\\\\Re',\n",
       " 314: '\\\\left\\\\|',\n",
       " 315: '\\\\tilde',\n",
       " 316: '\\\\vphantom',\n",
       " 317: '\\\\ldots',\n",
       " 318: '\\\\crcr',\n",
       " 319: '!',\n",
       " 320: '\\\\thinspace',\n",
       " 321: '\\\\protecte',\n",
       " 322: '{',\n",
       " 323: '\\\\right.',\n",
       " 324: '\\\\ddots',\n",
       " 325: '\\\\right)',\n",
       " 326: '\\\\Longrightarrow',\n",
       " 327: '\\\\dag',\n",
       " 328: '\\\\ss',\n",
       " 329: 'ule',\n",
       " 330: '\\\\verb',\n",
       " 331: '\\\\protectZ',\n",
       " 332: '\\\\beta',\n",
       " 333: '\\\\O',\n",
       " 334: '\\\\vert',\n",
       " 335: '\\\\noalign',\n",
       " 336: '\\\\d',\n",
       " 337: '\\\\oplus',\n",
       " 338: 'f',\n",
       " 339: 'w',\n",
       " 340: '\\\\rightarrowfill',\n",
       " 341: '\\\\mskip',\n",
       " 342: '\\\\hookrightarrow',\n",
       " 343: '\\\\mathstrut',\n",
       " 344: '3.1',\n",
       " 345: 'j',\n",
       " 346: '\\\\sum',\n",
       " 347: '\\\\Pi',\n",
       " 348: '\\\\in',\n",
       " 349: '\\\\sqrt',\n",
       " 350: '5',\n",
       " 351: '\\\\cap',\n",
       " 352: '\\\\fboxsep',\n",
       " 353: '\\\\rightharpoonup',\n",
       " 354: '\\\\upsilon',\n",
       " 355: '\\\\iota',\n",
       " 356: '\\\\hspace',\n",
       " 357: '\\\\mathcal',\n",
       " 358: '\\\\phantom',\n",
       " 359: '\\\\vline',\n",
       " 360: '\\\\nabla',\n",
       " 361: '\\\\biggr',\n",
       " 362: 'M',\n",
       " 363: '\\\\right\\\\rangle',\n",
       " 364: '\\\\c',\n",
       " 365: '\\\\biggm',\n",
       " 366: '\\\\,',\n",
       " 367: '\\\\leq',\n",
       " 368: '\\\\it',\n",
       " 369: '\\\\Biggm',\n",
       " 370: '\\\\:',\n",
       " 371: '\\\\put',\n",
       " 372: '\\\\textrm',\n",
       " 373: '\\\\neq',\n",
       " 374: '\\\\rfloor',\n",
       " 375: '\\\\rlap',\n",
       " 376: '\\\\left\\\\lbrack',\n",
       " 377: '\\\\right\\\\rceil',\n",
       " 378: '\\\\bar',\n",
       " 379: '\\\\footnotesize',\n",
       " 380: '\\\\vrule',\n",
       " 381: '\\\\odot',\n",
       " 382: 'B',\n",
       " 383: '\\\\bigr',\n",
       " 384: '\\\\lbrack',\n",
       " 385: '\\\\le',\n",
       " 386: '\\\\Large',\n",
       " 387: '0.4',\n",
       " 388: '\\\\Bigl',\n",
       " 389: '\\\\cal',\n",
       " 390: '\\\\setlength',\n",
       " 391: 'p',\n",
       " 392: 'W',\n",
       " 393: '\\\\xi',\n",
       " 394: '\\\\textbf',\n",
       " 395: '\\\\sl',\n",
       " 396: '\\\\breve',\n",
       " 397: '\\\\Upsilon',\n",
       " 398: '\\\\land',\n",
       " 399: '\\\\circ',\n",
       " 400: '---',\n",
       " 401: '\\\\null',\n",
       " 402: '\\\\buildrel',\n",
       " 403: '\\\\mathit',\n",
       " 404: '\\\\diamondsuit',\n",
       " 405: 'Y',\n",
       " 406: '\\\\Leftrightarrow',\n",
       " 407: '\\\\Rightarrow',\n",
       " 408: '\\\\left<',\n",
       " 409: '\\\\setminus',\n",
       " 410: 'r',\n",
       " 411: '\\\\mathrel',\n",
       " 412: '\\\\tiny',\n",
       " 413: '\\\\mathop',\n",
       " 414: '_',\n",
       " 415: '\\\\geq',\n",
       " 416: '\\\\bigskip',\n",
       " 417: '\\\\cdots',\n",
       " 418: '\\\\epsilon',\n",
       " 419: '\\\\framebox',\n",
       " 420: '\\\\*',\n",
       " 421: '\\\\varpi',\n",
       " 422: 'in',\n",
       " 423: '\\\\emptyset',\n",
       " 424: '\\\\sf',\n",
       " 425: '\\\\displaystyle',\n",
       " 426: '\\\\bigl',\n",
       " 427: '\\\\vee',\n",
       " 428: '\\\\succeq',\n",
       " 429: '\\\\notin',\n",
       " 430: '+',\n",
       " 431: '\\\\o',\n",
       " 432: '\\\\pi',\n",
       " 433: 'a',\n",
       " 434: '\\\\Psi',\n",
       " 435: '1',\n",
       " 436: '\\\\root',\n",
       " 437: '\\\\multicolumn',\n",
       " 438: '\\\\doteq',\n",
       " 439: '\\\\hfil',\n",
       " 440: '\\\\succ',\n",
       " 441: '\\\\cdotp',\n",
       " 442: '\\\\varsigma',\n",
       " 443: '\\\\line',\n",
       " 444: '\\\\right\\\\rbrace',\n",
       " 445: '\\\\gg',\n",
       " 446: '*',\n",
       " 447: '20',\n",
       " 448: '\\\\}',\n",
       " 449: 'Object]',\n",
       " 450: '\\\\nearrow',\n",
       " 451: 'C',\n",
       " 452: '\\\\sigma',\n",
       " 453: '\\\\textup',\n",
       " 454: '\\\\oint',\n",
       " 455: '\\\\hat',\n",
       " 456: '0',\n",
       " 457: '\\\\right/',\n",
       " 458: '\\\\ni',\n",
       " 459: '\\\\zeta',\n",
       " 460: '\\\\right\\\\vert',\n",
       " 461: ']',\n",
       " 462: '\\\\underbrace',\n",
       " 463: '\\\\overbrace',\n",
       " 464: '`',\n",
       " 465: 'mm',\n",
       " 466: 'G',\n",
       " 467: '\\\\aleph',\n",
       " 468: '\\\\approx',\n",
       " 469: '\\\\top',\n",
       " 470: '=',\n",
       " 471: '\\\\sharp',\n",
       " 472: ';',\n",
       " 473: '\\\\textit',\n",
       " 474: '\\\\setcounter',\n",
       " 475: '\\\\vcenter',\n",
       " 476: ':',\n",
       " 477: '\\\\mathclose',\n",
       " 478: 'g',\n",
       " 479: '\\\\relax',\n",
       " 480: '\\\\brack',\n",
       " 481: '\\\\natural',\n",
       " 482: ',',\n",
       " 483: '8.5',\n",
       " 484: 'F',\n",
       " 485: '\\\\do',\n",
       " 486: '\\\\itshape',\n",
       " 487: '\\\\forall',\n",
       " 488: '\\\\kappa',\n",
       " 489: '\\\\ne',\n",
       " 490: '\\\\leftrightarrow',\n",
       " 491: '\\\\wedge',\n",
       " 492: '\\\\quad',\n",
       " 493: '\\\\bigtriangleup',\n",
       " 494: '\\\\ref',\n",
       " 495: '\\\\lbrace',\n",
       " 496: '\\\\prec',\n",
       " 497: '\\\\bigcup',\n",
       " 498: '\\\\of',\n",
       " 499: '[object',\n",
       " 500: '\\\\right\\\\rbrack',\n",
       " 501: '\\\\imath',\n",
       " 502: '\\\\jmath',\n",
       " 503: 'c',\n",
       " 504: '\\\\supseteq',\n",
       " 505: '\\\\mathtt',\n",
       " 506: '--',\n",
       " 507: '\\\\&',\n",
       " 508: '\\\\hphantom',\n",
       " 509: '\\\\textsf',\n",
       " 510: '\\\\nu',\n",
       " 511: '|',\n",
       " 512: '\\\\searrow',\n",
       " 513: '\\\\lq',\n",
       " 514: '\\\\bigvee',\n",
       " 515: '\\\\lambda',\n",
       " 516: '\\\\smash',\n",
       " 517: '\\\\brace',\n",
       " 518: 'I',\n",
       " 519: '\\\\right\\\\|',\n",
       " 520: '\\\\medskip',\n",
       " 521: '\\\\bmod',\n",
       " 522: '\\\\right\\\\}',\n",
       " 523: '\\\\scriptsize',\n",
       " 524: '\\\\frac',\n",
       " 525: '\\\\ae',\n",
       " 526: '\\\\vartheta',\n",
       " 527: '\\\\left\\\\{',\n",
       " 528: '\\\\left\\\\lceil',\n",
       " 529: 'T',\n",
       " 530: '\\\\protect',\n",
       " 531: 'q',\n",
       " 532: '\\\\Omega',\n",
       " 533: 'i',\n",
       " 534: '\\\\lceil',\n",
       " 535: '\\\\strut',\n",
       " 536: '\\\\/',\n",
       " 537: '\\\\operatorname',\n",
       " 538: '\\\\!',\n",
       " 539: '\\\\left.',\n",
       " 540: '\\\\right[',\n",
       " 541: '\\\\theta',\n",
       " 542: '\\\\longmapsto',\n",
       " 543: '\\\\ensuremath',\n",
       " 544: '\\\\rho',\n",
       " 545: '\\\\mkern',\n",
       " 546: '\\\\langle',\n",
       " 547: '\\\\right>',\n",
       " 548: '\\\\atop'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = {word_to_index[w]:w for w in word_to_index}\n",
    "index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform image array\n",
    "tf_resize_normalize = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = df.iloc[0,0]\n",
    "imgaa = Image.open(f'./col_774_A4_2023/SyntheticData/images/{img}')\n",
    "aa = tf_resize_normalize(imgaa)\n",
    "type(imgaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- useless --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formulate(f, w2i=word_to_index):\n",
    "    f = f.split()\n",
    "    f = ['<SOF>']+f+['<EOF>']\n",
    "    f_idx = [w2i[word] for word in f]\n",
    "    f_tensor = torch.as_tensor(f_idx)\n",
    "\n",
    "    return f_tensor\n",
    "\n",
    "def add_formula_padded(df):\n",
    "    # TODO: doesn't work some some nasty reason\n",
    "    # formula = df['formula'].map(lambda x: formulate(x))\n",
    "    \n",
    "    formula = []\n",
    "    for f in df['formula']:\n",
    "        formula.append(formulate(f))\n",
    "\n",
    "    fk = pad_sequence(formula)\n",
    "    fk = fk.T.tolist()\n",
    "    fk = list(map(lambda x: torch.tensor(x), fk))\n",
    "    # for i in range(len(fk)):\n",
    "    #     fk[i] = torch.tensor(fk[i])\n",
    "\n",
    "    # df[col] = fk\n",
    "    return fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulas_padded = add_formula_padded(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([629])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_formula = formulas_padded[0].shape\n",
    "max_len_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = add_formula_padded(df)\n",
    "# aa[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- useless --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquationDataset(Dataset):\n",
    "    def __init__(self, csv_file, word2idx=word_to_index, transform=tf_resize_normalize):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        # self.formulas_padded = add_formula_padded(self.data)\n",
    "        self.transform = transform\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     formula = self.formulas[idx]\n",
    "    #     tokens = formula.split()\n",
    "    #     indices = [self.word_to_index[word] for word in tokens]\n",
    "    #     return torch.tensor(indices)\n",
    "\n",
    "    def __getitem__(self, idx, directory='SyntheticData'):\n",
    "        img_name = self.data.iloc[idx, 0]\n",
    "        image = Image.open(f\"./col_774_A4_2023/{directory}/images/{img_name}\")\n",
    "        \n",
    "        formula = self.data.iloc[idx, 1].split()\n",
    "        formula = ['<SOF>'] + formula + ['<EOF>']\n",
    "        formula_tensor = torch.tensor([self.word2idx[word] for word in formula])\n",
    "        \n",
    "        pad = nn.ConstantPad1d((0, 629-formula_tensor.shape[0]), self.word2idx['<PAD>'])\n",
    "        formula_tensor = pad(formula_tensor).T\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "\n",
    "        return image_tensor, formula_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1, 239, 163,  26, 322, 288, 506,  15,  27, 322, 322,  50,  96,  96,\n",
       "         96, 265, 414, 322,  27, 322, 322,  50,  96,  96,  96,  26, 322, 533,\n",
       "         96, 470, 456, 482, 213, 410,  26, 322, 288, 430, 430,  15,  27, 322,\n",
       "        322,  50,  96,  96,  96, 265, 414, 322,  27, 322, 322,  50,  96,  96,\n",
       "         96,  26, 322, 533,  96, 470, 456,  77, 187, 213, 239,   2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = df.iloc[1, 1].split()\n",
    "formula = ['<SOF>'] + formula + ['<EOF>']\n",
    "formula_tensor = torch.tensor([word_to_index[word] for word in formula])\n",
    "\n",
    "formula_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Large \\theta\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[386], index_to_word[541])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EquationDataset(\"./col_774_A4_2023/SyntheticData/train.csv\")\n",
    "test_data = EquationDataset(\"./col_774_A4_2023/SyntheticData/test.csv\")\n",
    "val_data = EquationDataset(\"./col_774_A4_2023/SyntheticData/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROB: using collate fn on tuple dataset\n",
    "# used lambda fn to collate on first tensor of tuple\n",
    "\n",
    "# def custom_padseq(batch):\n",
    "#     print([i[1].shape for i in batch])\n",
    "    # print(batch[0][1].shape)\n",
    "\n",
    "    # transposed = list(zip(*batch))\n",
    "    # padded_tensor = pad_sequence(transposed[1], batch_first=True, padding_value=0)\n",
    "    # return transposed[0] + [padded_tensor]\n",
    "\n",
    "trial_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_15040\\3023238869.py:26: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  formula_tensor = pad(formula_tensor).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "\n",
      "balle\n",
      "\n",
      "torch.Size([32, 629])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "balle\n",
      "\n",
      "torch.Size([32, 629])\n"
     ]
    }
   ],
   "source": [
    "# expected Tensor as element 0 in argument 0, but got tuple\n",
    "\n",
    "for k, (i,j) in enumerate(trial_loader):\n",
    "    print(type(i))\n",
    "    print(\"\\nballe\\n\")\n",
    "    print(j.shape)\n",
    "    \n",
    "    if k==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncDec(vocab, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: EncDec, num_epochs=10, lr=0.01, batch_size=50, criterion=nn.CrossEntropyLoss(), save=True, device=\"cuda\"):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        if save:\n",
    "            torch.save(model, \"./mkc.pth\")\n",
    "            # torch.save(model.encoder.state_dict(), \"./encoder.pth\")\n",
    "            # torch.save(model.decoder.state_dict(), \"./decoder.pth\")\n",
    "\n",
    "        for i, (img_seq, target_seq) in enumerate(train_loader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # encoder.zero_grad()\n",
    "            # decoder.zero_grad()\n",
    "\n",
    "            img_seq = img_seq.float()\n",
    "            img_seq = img_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            context = model.encoder.forward(img_seq)\n",
    "\n",
    "            # hidden = torch.zeros(1, batch_size, hidden_size).to(device)\n",
    "            # cell = torch.zeros(1, batch_size, hidden_size).to(device)\n",
    "            hidden = None\n",
    "\n",
    "            loss = 0\n",
    "            #embedding where?\n",
    "            output_seq = torch.tensor([[model.word_to_index['<SOF>']]] * batch_size).to(device)\n",
    "            # print(output_seq.shape)\n",
    "            output_thru = 0\n",
    "\n",
    "            # print(f'target-seq {target_seq.shape}')\n",
    "            for target_tok in target_seq.split(1, dim=1):\n",
    "                # output, hidden, cell = decoder(context, output_seq)\n",
    "                print(context.shape, output_seq.shape)\n",
    "                output, hidden = model.decoder.forward(context, output_seq, hidden)\n",
    "\n",
    "                # print(f'target-tok {target_tok.shape}')\n",
    "                # print(f'outpt {output.shape}')\n",
    "\n",
    "                # 50% teacher enforcing\n",
    "                if torch.rand(1, 1) < 0.5:\n",
    "                    output_seq = target_tok\n",
    "                    # print('1,', output_seq.shape)\n",
    "                else:\n",
    "                    output_seq = output.argmax(2)\n",
    "                    # print('2,',output_seq.shape)\n",
    "\n",
    "                output = output.squeeze()\n",
    "                output = output.unsqueeze(2)\n",
    "                # print(f'outpt after sqiunsq {output.shape}')\n",
    "\n",
    "                if type(output_thru) == int:\n",
    "                    output_thru = output\n",
    "                else:\n",
    "                    output_thru = torch.cat((output_thru, output), dim = 2)\n",
    "\n",
    "\n",
    "            # for t in range(target_seq.shape[1]):\n",
    "\n",
    "            #     # output, hidden, cell = decoder(context, output_seq)\n",
    "            #     output, hidden = model.decoder.forward(context, output_seq, hidden)\n",
    "\n",
    "            #     # print(output.shape)\n",
    "            #     # print(target_seq[:,t].shape)\n",
    "            #     # target_seq_prob = torch.tensor()\n",
    "\n",
    "            #     # 50% teacher enforcing\n",
    "            #     if torch.rand(1,1) < 0.5:\n",
    "            #         output_seq = target_seq[:, t].unsqueeze(1)\n",
    "            #         # print('1,', output_seq.shape)\n",
    "            #     else: \n",
    "            #         output_seq = output.argmax(2)\n",
    "            #         # print('2,',output_seq.shape)\n",
    "\n",
    "            #     output = output.squeeze()\n",
    "            #     output = output.unsqueeze(2)\n",
    "            #     if type(output_thru) == int:\n",
    "            #         output_thru = output\n",
    "            #     else:\n",
    "            #         output_thru = torch.cat((output_thru, output), dim = 2)\n",
    "\n",
    "\n",
    "            # print(target_seq.shape)\n",
    "            # print(output_thru.shape)\n",
    "            loss += criterion(output_thru, target_seq)\n",
    "            # print(loss)\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                # 75000/32 = 2343.75\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                if (i+1)%1000 == 0 and save:\n",
    "                    torch.save(model, \"./mkc.pth\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch: [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n",
      "torch.Size([50, 512, 1, 1]) torch.Size([50, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m modelly \u001b[39m=\u001b[39m EncDec(vocab, word_to_index, index_to_word)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(modelly)\n",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m         \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m save:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m             torch\u001b[39m.\u001b[39msave(model, \u001b[39m\"\u001b[39m\u001b[39m./mkc.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X66sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\DevTools\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\DevTools\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelly = EncDec(vocab, word_to_index, index_to_word)\n",
    "train(modelly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Step [10/1500], Loss: 2.7699\n",
      "Epoch 1/2, Step [20/1500], Loss: 0.9682\n",
      "Epoch 1/2, Step [30/1500], Loss: 0.8340\n",
      "Epoch 1/2, Step [40/1500], Loss: 0.7838\n",
      "Epoch 1/2, Step [50/1500], Loss: 0.6893\n",
      "Epoch 1/2, Step [60/1500], Loss: 0.6919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 37\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# print(f'target-seq {target_seq.shape}')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m target_tok \u001b[39min\u001b[39;00m target_seq\u001b[39m.\u001b[39msplit(\u001b[39m1\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# output, hidden, cell = decoder(context, output_seq)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     output, hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mforward(context, output_seq, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# print(f'target-tok {target_tok.shape}')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# print(f'outpt {output.shape}')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# 50% teacher enforcing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m \u001b[39m0.5\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(\"Context\", context.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# print(\"Embed\", embedded.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m embedded \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((context, embedded), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embedded, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(\"Out\", output.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# output = output.squeeze(1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X51sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(output)\n",
      "File \u001b[1;32mc:\\DevTools\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\DevTools\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, num_epochs=2, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, num_epochs=2, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(list(encoder.parameters()) +\n",
    "#                        list(decoder.parameters()), lr=0.01)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for i, (input_images, target_formulas) in enumerate(data_loader):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         context_vectors = encoder(input_images)\n",
    "#         # Initialize the hidden state for the decoder\n",
    "#         hidden = torch.zeros(1, 32, 512)\n",
    "#         # Initialize the cell state for the decoder\n",
    "#         cell = torch.zeros(1, 32, 512)\n",
    "#         # Initialize the input sequence with the <sos> token\n",
    "#         output_seq = torch.tensor([[output_vocab.stoi['<sos>']] * batch_size])\n",
    "\n",
    "#         use_teacher_forcing = True if torch.random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "#         loss = 0\n",
    "#         for t in range(1, target_formulas.shape[1]):\n",
    "#             output, hidden, cell = decoder(\n",
    "#                 output_seq, hidden, cell, context_vectors)\n",
    "#             if use_teacher_forcing:\n",
    "#                 input_seq = target_formulas[:, t].unsqueeze(0)\n",
    "#             else:\n",
    "#                 input_seq = output.argmax(1)\n",
    "#             loss += criterion(output, target_formulas[:, t])\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if i % print_every == 0:\n",
    "#             print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "#                   .format(epoch+1, num_epochs, i, len(data_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=tf_resize_normalize):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx, directory='SyntheticData'):\n",
    "        img_name = self.data.iloc[idx, 0]\n",
    "        image = Image.open(f\"./col_774_A4_2023/{directory}/images/{img_name}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "\n",
    "        return image_tensor\n",
    "    \n",
    "\n",
    "test_data = TestDataset(\"./col_774_A4_2023/SyntheticData/test.csv\")\n",
    "val_data = TestDataset(\"./col_774_A4_2023/SyntheticData/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: EncDec, dir_folder='SyntheticData', dir_data='test', device='cuda', batch_size=100):\n",
    "    # loader = pd.read_csv(f'./col_774_A4_2023/{dir_folder}/{dir_data}.csv')\n",
    "    \n",
    "    if dir_data == 'test':\n",
    "        loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    elif dir_data == 'train':\n",
    "        loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    final_latex = []\n",
    "    for i, (images) in enumerate(loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        context_vectors = model.encoder.forward(images)\n",
    "        hidden = None\n",
    "\n",
    "        predicted_tokens = []\n",
    "        # output_seq = torch.tensor([[model.word_to_index['<SOF>']]] * batch_size).to(device)\n",
    "        input_token = torch.tensor([[model.word_to_index['<SOF>']]]*batch_size).to(device)\n",
    "\n",
    "        for i in range(model.out_size):\n",
    "            # print(context_vectors.shape)\n",
    "            # print(input_token.shape)\n",
    "            output, hidden = model.decoder.forward(context_vectors, input_token, hidden)\n",
    "            \n",
    "            predicted_token = output.argmax(dim=2)\n",
    "            predicted_tokens.append(predicted_token)\n",
    "\n",
    "            input_token = predicted_token\n",
    "            \n",
    "        for j in range(len(predicted_tokens)):\n",
    "            predicted_latex = []\n",
    "            for i in range(batch_size):\n",
    "                #    print(predicted_tokens[j].shape)\n",
    "                #    print(predicted_tokens[j][0,i].item())\n",
    "                predicted_latex.append(model.index_to_word[predicted_tokens[j][i, 0].item()])\n",
    "\n",
    "            final_latex.append(predicted_latex)\n",
    "\n",
    "    return final_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_csv \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m./col_774_A4_2023/SyntheticData/test.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_csv\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test_csv = pd.read_csv('./col_774_A4_2023/SyntheticData/test.csv')\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 8.00 GiB total capacity; 12.33 GiB already allocated; 0 bytes free; 14.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_predict \u001b[39m=\u001b[39m predict(model)\n",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     context_vectors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mforward(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     predicted_tokens \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32mc:\\Users\\aniru\\Documents\\VSCode\\COL774\\A4\\part1.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniru/Documents/VSCode/COL774/A4/part1.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n",
      "File \u001b[1;32mc:\\DevTools\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 8.00 GiB total capacity; 12.33 GiB already allocated; 0 bytes free; 14.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "test_predict = predict(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
