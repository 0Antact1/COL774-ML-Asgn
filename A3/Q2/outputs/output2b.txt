Learning rate = 0.01 ; Epochs = 200

Epoch 0/200 - Loss: 1.6209
Epoch 10/200 - Loss: 1.4060
Epoch 20/200 - Loss: 1.2169
Epoch 30/200 - Loss: 1.1327
Epoch 40/200 - Loss: 1.0849
Epoch 50/200 - Loss: 1.0526
Epoch 60/200 - Loss: 1.0283
Epoch 70/200 - Loss: 1.0088
Epoch 80/200 - Loss: 0.9923
Epoch 90/200 - Loss: 0.9780
Epoch 100/200 - Loss: 0.9653
Epoch 110/200 - Loss: 0.9538
Epoch 120/200 - Loss: 0.9435
Epoch 130/200 - Loss: 0.9339
Epoch 140/200 - Loss: 0.9252
Epoch 150/200 - Loss: 0.9170
Epoch 160/200 - Loss: 0.9094
Epoch 170/200 - Loss: 0.9022
Epoch 180/200 - Loss: 0.8955
Epoch 190/200 - Loss: 0.8891
Epoch 200/200 - Loss: 0.8831
For hidden_size: [1]:
Train scores:-
              precision    recall  f1-score   support

           1       0.99      0.70      0.82      2807
           2       0.49      0.64      0.56      1518
           3       0.48      0.61      0.53      1528
           4       0.27      0.52      0.35      1031
           5       0.91      0.61      0.73      3116

    accuracy                           0.63     10000
   macro avg       0.63      0.62      0.60     10000
weighted avg       0.74      0.63      0.66     10000

Test scores:-
              precision    recall  f1-score   support

           1       1.00      0.72      0.84       316
           2       0.47      0.66      0.55       140
           3       0.45      0.62      0.52       144
           4       0.33      0.50      0.40       121
           5       0.90      0.60      0.72       279

    accuracy                           0.64      1000
   macro avg       0.63      0.62      0.60      1000
weighted avg       0.73      0.64      0.67      1000

[1] done
Epoch 0/200 - Loss: 1.7289
Epoch 10/200 - Loss: 1.2760
Epoch 20/200 - Loss: 1.0666
Epoch 30/200 - Loss: 0.9573
Epoch 40/200 - Loss: 0.8895
Epoch 50/200 - Loss: 0.8456
Epoch 60/200 - Loss: 0.8153
Epoch 70/200 - Loss: 0.7932
Epoch 80/200 - Loss: 0.7763
Epoch 90/200 - Loss: 0.7631
Epoch 100/200 - Loss: 0.7525
Epoch 110/200 - Loss: 0.7438
Epoch 120/200 - Loss: 0.7366
Epoch 130/200 - Loss: 0.7304
Epoch 140/200 - Loss: 0.7252
Epoch 150/200 - Loss: 0.7206
Epoch 160/200 - Loss: 0.7166
Epoch 170/200 - Loss: 0.7130
Epoch 180/200 - Loss: 0.7098
Epoch 190/200 - Loss: 0.7069
Epoch 200/200 - Loss: 0.7043
For hidden_size: [5]:
Train scores:-
              precision    recall  f1-score   support

           1       0.94      0.85      0.89      2173
           2       0.70      0.70      0.70      1966
           3       0.54      0.59      0.56      1771
           4       0.44      0.54      0.48      1618
           5       0.82      0.69      0.75      2472

    accuracy                           0.69     10000
   macro avg       0.69      0.68      0.68     10000
weighted avg       0.71      0.69      0.69     10000

Test scores:-
              precision    recall  f1-score   support

           1       0.92      0.89      0.91       237
           2       0.69      0.68      0.68       201
           3       0.49      0.57      0.53       171
           4       0.48      0.50      0.49       179
           5       0.77      0.68      0.72       212

    accuracy                           0.68      1000
   macro avg       0.67      0.66      0.67      1000
weighted avg       0.69      0.68      0.68      1000

[5] done
Epoch 0/200 - Loss: 1.6529
Epoch 10/200 - Loss: 1.0917
Epoch 20/200 - Loss: 0.9417
Epoch 30/200 - Loss: 0.8683
Epoch 40/200 - Loss: 0.8236
Epoch 50/200 - Loss: 0.7939
Epoch 60/200 - Loss: 0.7730
Epoch 70/200 - Loss: 0.7578
Epoch 80/200 - Loss: 0.7463
Epoch 90/200 - Loss: 0.7374
Epoch 100/200 - Loss: 0.7303
Epoch 110/200 - Loss: 0.7245
Epoch 120/200 - Loss: 0.7196
Epoch 130/200 - Loss: 0.7155
Epoch 140/200 - Loss: 0.7119
Epoch 150/200 - Loss: 0.7087
Epoch 160/200 - Loss: 0.7058
Epoch 170/200 - Loss: 0.7031
Epoch 180/200 - Loss: 0.7006
Epoch 190/200 - Loss: 0.6983
Epoch 200/200 - Loss: 0.6960
For hidden_size: [10]:
Train scores:-
              precision    recall  f1-score   support

           1       0.93      0.85      0.89      2134
           2       0.70      0.70      0.70      1958
           3       0.57      0.59      0.58      1879
           4       0.43      0.55      0.48      1559
           5       0.82      0.70      0.75      2470

    accuracy                           0.69     10000
   macro avg       0.69      0.68      0.68     10000
weighted avg       0.71      0.69      0.70     10000

Test scores:-
              precision    recall  f1-score   support

           1       0.92      0.89      0.91       235
           2       0.69      0.69      0.69       198
           3       0.52      0.58      0.55       177
           4       0.48      0.50      0.49       178
           5       0.76      0.67      0.72       212

    accuracy                           0.68      1000
   macro avg       0.67      0.67      0.67      1000
weighted avg       0.69      0.68      0.68      1000

[10] done
Epoch 0/200 - Loss: 1.8486
Epoch 10/200 - Loss: 1.0218
Epoch 20/200 - Loss: 0.8837
Epoch 30/200 - Loss: 0.8242
Epoch 40/200 - Loss: 0.7898
Epoch 50/200 - Loss: 0.7675
Epoch 60/200 - Loss: 0.7521
Epoch 70/200 - Loss: 0.7409
Epoch 80/200 - Loss: 0.7325
Epoch 90/200 - Loss: 0.7259
Epoch 100/200 - Loss: 0.7205
Epoch 110/200 - Loss: 0.7161
Epoch 120/200 - Loss: 0.7123
Epoch 130/200 - Loss: 0.7090
Epoch 140/200 - Loss: 0.7061
Epoch 150/200 - Loss: 0.7033
Epoch 160/200 - Loss: 0.7008
Epoch 170/200 - Loss: 0.6984
Epoch 180/200 - Loss: 0.6961
Epoch 190/200 - Loss: 0.6937
Epoch 200/200 - Loss: 0.6914
For hidden_size: [50]:
Train scores:-
              precision    recall  f1-score   support

           1       0.92      0.87      0.89      2094
           2       0.72      0.70      0.71      2037
           3       0.56      0.59      0.57      1840
           4       0.45      0.55      0.49      1617
           5       0.81      0.71      0.76      2412

    accuracy                           0.69     10000
   macro avg       0.69      0.68      0.68     10000
weighted avg       0.71      0.69      0.70     10000

Test scores:-
              precision    recall  f1-score   support

           1       0.92      0.90      0.91       233
           2       0.70      0.67      0.68       205
           3       0.50      0.57      0.53       175
           4       0.50      0.50      0.50       187
           5       0.73      0.69      0.71       200

    accuracy                           0.68      1000
   macro avg       0.67      0.66      0.67      1000
weighted avg       0.68      0.68      0.68      1000

[50] done
Epoch 0/200 - Loss: 1.7228
Epoch 10/200 - Loss: 0.9930
Epoch 20/200 - Loss: 0.8638
Epoch 30/200 - Loss: 0.8100
Epoch 40/200 - Loss: 0.7796
Epoch 50/200 - Loss: 0.7602
Epoch 60/200 - Loss: 0.7468
Epoch 70/200 - Loss: 0.7370
Epoch 80/200 - Loss: 0.7296
Epoch 90/200 - Loss: 0.7238
Epoch 100/200 - Loss: 0.7191
Epoch 110/200 - Loss: 0.7151
Epoch 120/200 - Loss: 0.7117
Epoch 130/200 - Loss: 0.7087
Epoch 140/200 - Loss: 0.7060
Epoch 150/200 - Loss: 0.7036
Epoch 160/200 - Loss: 0.7014
Epoch 170/200 - Loss: 0.6992
Epoch 180/200 - Loss: 0.6972
Epoch 190/200 - Loss: 0.6953
Epoch 200/200 - Loss: 0.6934
For hidden_size: [100]:
Train scores:-
              precision    recall  f1-score   support

           1       0.91      0.87      0.89      2079
           2       0.72      0.69      0.71      2067
           3       0.55      0.59      0.57      1830
           4       0.46      0.55      0.50      1658
           5       0.80      0.71      0.75      2366

    accuracy                           0.69     10000
   macro avg       0.69      0.68      0.69     10000
weighted avg       0.71      0.69      0.70     10000

Test scores:-
              precision    recall  f1-score   support

           1       0.92      0.91      0.91       232
           2       0.70      0.67      0.69       207
           3       0.50      0.58      0.53       172
           4       0.51      0.49      0.50       194
           5       0.72      0.69      0.70       195

    accuracy                           0.68      1000
   macro avg       0.67      0.67      0.67      1000
weighted avg       0.68      0.68      0.68      1000

[100] done